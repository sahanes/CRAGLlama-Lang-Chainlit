{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install llama_index==0.10.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge ipykernel --update-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/AI-Maker-Space/DataRepository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sahane/MultiAgent/maven-course-old/assignments/week_02/proposal_generation_agent/workflowLlamaIndex\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U llama-index llama-index-tools-tavily-research llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "#----------------------------\n",
    "load_dotenv()\n",
    "\n",
    "VERSION = '1.0_rc3'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.environ[\"LANGCHAIN_PROJECT\"] + f\" - v. {VERSION}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2=True\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamaworkflow - v. 1.0_rc3\n"
     ]
    }
   ],
   "source": [
    "print(LANGCHAIN_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sahane/MultiAgent/maven-course-old/assignments/week_02/proposal_generation_agent/workflowLlamaIndex\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import CRAGLlamaIndexWorkflow\n",
    "# importlib.reload(CRAGLlamaIndexWorkflow)\n",
    "from CRAGLlamaIndexWorkflow import run_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_workflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     response\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[43mrun_workflow\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere the hallucination is more harmful?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_workflow' is not defined"
     ]
    }
   ],
   "source": [
    "# from CRAGLlamaIndexWorkflow import CorrectiveRAGLlamIndexWorkflow\n",
    "\n",
    "import asyncio\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response=asyncio.run(run_workflow(\"where the hallucination is more harmful?\"))\n",
    "    # print(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow initialized.\n",
      "\n",
      "Workflow run completed.\n",
      "\n",
      "\n",
      "Hallucinations can occur in healthcare due to a variety of reasons, including psychiatric, medical, and substance use-related causes. These causes can be linked to factors such as lesions in sensory pathways, irritative processes in cortical centers, abnormal brain activity, cognitive processes, and sensory impairments. Additionally, hallucinations can be influenced by conditions like neurological disorders, sensory deprivation, and certain medications.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    return await run_workflow(\"why hallucination happens in healthcare?\")\n",
    "\n",
    "# Get the current event loop; if there isn't one, it will create a new one\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Check if the loop is already running\n",
    "if not loop.is_running():\n",
    "    response = loop.run_until_complete(main())\n",
    "else:\n",
    "    response = await main()  # This should be used in an async environment\n",
    "\n",
    "# If you're in a synchronous block and the loop is running, use create_task or ensure_future\n",
    "# response = asyncio.create_task(main())  # This will not block and will return a Future object\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow initialized.\n",
      "\n",
      "Workflow run completed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Llama2 was pretrained using a method referred to as second-stage pretraining on any pretraining dataset, utilizing the standard Llama2-7B architecture, which is a decoder-only transformer model with RoPE."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "# CRAGAgent = CRAGLlamIndexWorkflow()\n",
    "response = await run_workflow(\n",
    "    query_str=\"How was Llama2 pretrained?\",\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# import asyncio\n",
    "# async def main():\n",
    "#     documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "#     CRAGAgent = CorrectiveRAGLlamIndexWorkflow()\n",
    "#     print(\"Workflow initialized\")\n",
    "#     try:\n",
    "#         index = await CRAGAgent.run(documents=documents)\n",
    "#         print(\"Workflow run completed\")\n",
    "#         return index\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during workflow execution: {e}\")\n",
    "#         raise\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CRAGLlamaIndexWorkflow import CorrectiveRAGLlamIndexWorkflow\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# from IPython.display import Markdown, display\n",
    "# import asyncio\n",
    "# # documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "# # CRAGAgent = CorrectiveRAGLlamIndexWorkflow()\n",
    "# # index = await CRAGAgent.run(documents=documents)\n",
    "# # nest_asyncio.apply()\n",
    "# async def main():\n",
    "#     documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "#     CRAGAgent = CorrectiveRAGLlamIndexWorkflow()\n",
    "#     index = await CRAGAgent.run(documents=documents)\n",
    "    \n",
    "\n",
    "#     # response = await CRAGAgent.run(\n",
    "#     #     query_str=\"How was Llama2 pretrained?\",\n",
    "#     #     index=index,\n",
    "#     #     tavily_ai_apikey=tavily_ai_api_key,\n",
    "#     # )\n",
    "#     return index#display(Markdown(str(response)))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ingest Data from AImakerspace repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crag_workflow.html\", \"r\") as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DataRepository'...\n",
      "remote: Enumerating objects: 90, done.\u001b[K\n",
      "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
      "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
      "remote: Total 90 (delta 24), reused 29 (delta 8), pack-reused 8 (from 1)\u001b[K\n",
      "Receiving objects: 100% (90/90), 70.26 MiB | 2.77 MiB/s, done.\n",
      "Resolving deltas: 100% (24/24), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/AI-Maker-Space/DataRepository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data\n",
    "! cp -r DataRepository/eu_ai_act.html data/eu_ai_act.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importing the LlamaGraph workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Workflow run completed.\n",
      "\n",
      "crag_workflow.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gio: file:///home/sahane/MultiAgent/maven-course-old/assignments/week_02/proposal_generation_agent/workflowLlamaIndex/crag_workflow.html: No application is registered as handling this file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top three main points in the EU ACT for artificial intelligence are the establishment of common rules for high-risk AI systems, the need for a Union legal framework to regulate AI systems in the internal market, and the harmonized rules for placing on the market, putting into service, and using high-risk AI systems.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "import sys\n",
    "sys.path.append('./workflowLlamaIndex/CRAGLlamaIndexWorkflow')\n",
    "from CRAGLlamaIndexWorkflow import CorrectiveRAGLlamIndexWorkflow\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CRAGAgent=CorrectiveRAGLlamIndexWorkflow()\n",
    "\n",
    "async def main():\n",
    "    documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "    print(len(documents))\n",
    "    try: \n",
    "        index = await CRAGAgent.run(documents=documents)\n",
    "        print(\"Workflow run completed.\\n\")\n",
    "\n",
    "        draw_all_possible_flows(\n",
    "        CRAGAgent, filename=\"crag_workflow.html\")\n",
    "\n",
    "        response = await CRAGAgent.run(\n",
    "            query_str=\"what are the top three main points in EU ACT for artificial intelligence?\",\n",
    "            index=index,\n",
    "            tavily_ai_apikey=os.getenv(\"TAVILY_API_KEY\"),\n",
    "        )\n",
    "        print()\n",
    "        return str(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during workflow execution: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sa=asyncio.run(main())\n",
    "    print(str(sa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Importing the async run for the result ONLY when already data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow initialized.\n",
      "\n",
      "Workflow run completed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The number of parameters in a 'Large Language Model' can vary widely depending on the specific architecture and implementation, but it generally reflects the model’s complexity and the amount of data it has been trained on."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "importlib.reload(CRAGLlamaIndexWorkflow)\n",
    "from CRAGLlamaIndexWorkflow import run_workflow\n",
    "\n",
    "response = await run_workflow(\n",
    "    query_str=\"How many parameters make a 'Large Language Model'?\",\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamaworkflow - v. 1.0_rc3\n"
     ]
    }
   ],
   "source": [
    "print(LANGCHAIN_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo2224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
